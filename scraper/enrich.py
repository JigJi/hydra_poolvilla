import asyncio
import re
import random
from datetime import datetime, timedelta

# --- Third-party Libs ---
from bs4 import BeautifulSoup
from sqlalchemy import select, update, func
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy import cast
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from playwright.async_api import async_playwright


# --- Local Models ---
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå models.py ‡πÅ‡∏•‡∏∞ database.py ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏¥‡∏°‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
from models import Villa
from database import DATABASE_URL

# ==========================================
# 1. CONFIGURATION & DATABASE SETUP
# ==========================================
CONFIG = {
    "MIN_SLEEP": 2,
    "MAX_SLEEP": 5,
    "CONCURRENT_TABS": 5,
    "TIMEOUT": 60000,
    "USER_AGENT": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
}

ASYNC_DATABASE_URL = DATABASE_URL.replace("postgresql://", "postgresql+asyncpg://")

# Database Engine Setup (Optimized for pgbouncer)
engine = create_async_engine(
    ASYNC_DATABASE_URL,
    echo=False,
    pool_pre_ping=True,
    connect_args={
        "statement_cache_size": 0,          
        "prepared_statement_cache_size": 0, 
        "command_timeout": 60
    }
)
AsyncSessionLocal = sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)


# ==========================================
# 2. UTILITY & STEALTH TOOLS
# ==========================================
class ScraperUtils:
    @staticmethod
    def clean_text(text):
        """Clean whitespace and strip text."""
        if not text: return ""
        return re.sub(r'\s+', ' ', text).strip()

    @staticmethod
    async def apply_stealth(page):
        """Inject scripts to evade bot detection."""
        await page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            window.chrome = { runtime: {} };
            Object.defineProperty(navigator, 'languages', { get: () => ['th-TH', 'th', 'en-US', 'en'] });
            Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
        """)


# ==========================================
# 3. PARSING LOGIC (Final Boss Edition ü¶æ)
# ==========================================
class BookingParser:
    
    @staticmethod
    def _parse_room_table_logic(soup):
        """
        Logic ‡πÅ‡∏Å‡∏∞ Room Table (Final Polish ‚ú®)
        - Name: ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏Å‡∏ß‡∏≤‡∏î Text ‡∏î‡∏¥‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏ö‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ï‡∏µ‡∏¢‡∏á‡∏≠‡∏≠‡∏Å
        - Bedrooms: ‡∏ô‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô Layout ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏≤
        - MaxGuests: ‡∏¢‡∏∂‡∏î‡∏ï‡∏≤‡∏° Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡πÅ‡∏•‡πâ‡∏ß
        """
        print("   üíé Scanning Room Table (Final Polish)...")
        
        data = {
            "name": "", "priceDaily": 0, "currency": "THB",
            "specs": {"bedrooms": 0, "bathrooms": 1, "maxGuests": 2},
            "layout": {"rooms": [], "others": []}
        }

        # 1. ‡∏´‡∏≤ Table
        table = soup.select_one('#hprt-table')
        if not table:
            rows = soup.select('table.hprt-table tbody > tr')
        else:
            rows = table.select('tbody > tr')
        
        if not rows: return data

        # 2. Smart Selection
        best_row = None
        best_score = -1
        
        for row in rows:
            cols = row.select('th, td')
            if len(cols) < 2: continue 
            
            # Scoring
            temp_name = ""
            name_el = cols[0].select_one('.hprt-roomtype-icon-link, .hprt-roomtype-link')
            if name_el: temp_name = ScraperUtils.clean_text(name_el.get_text())
            
            score = 0
            if "Villa" in temp_name or "‡∏ß‡∏¥‡∏•‡∏•‡∏≤" in temp_name: score += 100
            if "Pool" in temp_name: score += 50
            if "Private" in temp_name: score += 30
            if score > best_score:
                best_score = score
                best_row = row
        
        if not best_row: best_row = rows[0]

        # 3. Extraction
        cols = best_row.select('th, td')
        col_name = cols[0]
        
        # --- üõèÔ∏è Collect Layout First (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ‡∏ô‡∏±‡∏ö‡∏´‡πâ‡∏≠‡∏á‡∏ô‡∏≠‡∏ô) ---
        bed_items = []
        # ‡πÄ‡∏Å‡πá‡∏ö Text ‡πÄ‡∏ï‡∏µ‡∏¢‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏´‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á
        bed_texts_to_remove = [] 
        
        bed_containers = col_name.select('.rt-bed-type, .bedroom_bed_type, .room-config li')
        if bed_containers:
            for item in bed_containers:
                txt = ScraperUtils.clean_text(item.get_text())
                if txt: 
                    bed_items.append(txt)
                    bed_texts_to_remove.append(txt)
        else:
            bed_wrapper = col_name.select_one('.hprt-roomtype-bed, .bed-types-wrapper')
            if bed_wrapper:
                txt = ScraperUtils.clean_text(bed_wrapper.get_text())
                if txt: 
                    bed_items.append(txt)
                    bed_texts_to_remove.append(txt)

        data['layout']['rooms'] = list(set(bed_items))

        # --- üè∑Ô∏è Name Extraction (Cleanup Mode) ---
        name_el = col_name.select_one('.hprt-roomtype-icon-link, .hprt-roomtype-link, a.hprt-roomtype-link')
        if name_el:
            data['name'] = ScraperUtils.clean_text(name_el.get_text())
        
        # Fallback: ‡∏ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤‡∏á ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏≤ Text ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á ‡∏•‡∏ö‡∏î‡πâ‡∏ß‡∏¢ Text ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ï‡∏µ‡∏¢‡∏á
        if not data['name']:
            full_text = ScraperUtils.clean_text(col_name.get_text(" "))
            for bed_txt in bed_texts_to_remove:
                full_text = full_text.replace(bed_txt, "") # ‡∏•‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ï‡∏µ‡∏¢‡∏á‡∏≠‡∏≠‡∏Å
            
            # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏Ç‡∏¢‡∏∞‡∏≠‡∏∑‡πà‡∏ô‡πÜ
            full_text = full_text.replace("‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î:", "").strip()
            data['name'] = full_text[:100].strip() # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 100 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å‡∏û‡∏≠

        # --- üî¢ Bedrooms Count (Smart Count) ---
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏´‡πâ‡∏≠‡∏á‡∏ô‡∏≠‡∏ô" ‡∏´‡∏£‡∏∑‡∏≠ "Bedroom" ‡πÉ‡∏ô layout
        bedroom_count = 0
        for room in data['layout']['rooms']:
            if "‡∏´‡πâ‡∏≠‡∏á‡∏ô‡∏≠‡∏ô" in room or "Bedroom" in room:
                bedroom_count += 1
        
        if bedroom_count > 0:
            data['specs']['bedrooms'] = bedroom_count
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤‡∏´‡πâ‡∏≠‡∏á‡∏ô‡∏≠‡∏ô‡πÉ‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏´‡πâ‡∏≠‡∏á
            match = re.search(r'(\d+)\s*(?:‡∏´‡πâ‡∏≠‡∏á‡∏ô‡∏≠‡∏ô|Bedroom)', data['name'], re.IGNORECASE)
            if match:
                data['specs']['bedrooms'] = int(match.group(1))
            elif len(data['layout']['rooms']) > 0:
                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏ï‡∏µ‡∏¢‡∏á‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡πâ‡∏≠‡∏á‡∏ô‡∏≠‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 1 (Studio)
                data['specs']['bedrooms'] = 1

        # --- Guests & Price ---
        if len(cols) > 1:
            col_guest = cols[1]
            raw_text = ScraperUtils.clean_text(col_guest.get_text())
            match_num = re.search(r'[x√ó]\s*(\d+)', raw_text)
            if match_num:
                data['specs']['maxGuests'] = int(match_num.group(1))
            else:
                icons = col_guest.select('.bicon-occupancy, .bicon-person, i')
                valid = [i for i in icons if 'occupancy' in str(i) or 'person' in str(i)]
                if valid: data['specs']['maxGuests'] = len(valid)

        if len(cols) > 2:
            col_price = cols[2]
            price_text = ScraperUtils.clean_text(col_price.get_text())
            cur_price = col_price.select_one('.bui-price-display__value')
            if cur_price: price_text = cur_price.get_text()
            
            digits = re.findall(r'(\d{1,3}(?:,\d{3})*)', price_text)
            valid_prices = [int(d.replace(',', '')) for d in digits if int(d.replace(',', '')) > 500]
            if valid_prices: data['priceDaily'] = valid_prices[0]

        return data

    @staticmethod
    def _parse_facilities_logic(soup):
        print("   üèä‚Äç‚ôÇÔ∏è Scanning Facilities (Precision Mode)...")
        data = {
            "score": None,
            "popular": [],
            "categories": []
        }

        # 1. Score - ‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏ó‡∏µ‡πà div ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ï‡πâ h2 ‡πÉ‡∏ô section ‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        # ‡∏à‡∏≤‡∏Å HTML: <div class="da8a6fe12c fb14de7f14">9.7 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô...</div>
        score_section = soup.select_one('#hp_facilities_box .da8a6fe12c')
        if score_section:
            m = re.search(r'(\d+\.?\d*)', score_section.get_text())
            if m: 
                data['score'] = float(m.group(1))
                # print(f"      ‚úÖ Found Score: {data['score']}")

        # 2. Popular Facilities - ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        pop_items = []
        for el in soup.select('[data-testid="property-most-popular-facilities-wrapper"] .f6b6d2a959'):
            pop_items.append(el.get_text(strip=True))
        data['popular'] = list(set(pop_items))

        # 3. Categories - ‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏û‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏á (Sibling)
        # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡∏≠‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô h3 ‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÜ ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ ul
        all_group_containers = soup.select('[data-testid="facility-group-container"]')
        
        for container in all_group_containers:
            # ‡∏´‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ (h3)
            h3 = container.select_one('h3')
            if not h3: continue
            
            # Clean ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Text ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤ Icon ‡πÉ‡∏ô SVG
            # ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ: ‡∏î‡∏∂‡∏á text ‡∏à‡∏≤‡∏Å div ‡∏ä‡∏±‡πâ‡∏ô‡πÉ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏°‡∏ß‡∏î
            category_name = h3.select_one('.e7addce19e, .d31c9df771')
            category_name = category_name.get_text(strip=True) if category_name else h3.get_text(strip=True)

            items = []
            
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô list (li)
            # ‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡πÑ‡∏õ‡∏ó‡∏µ‡πà span class .f6b6d2a959 ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏™‡πà‡∏á‡∏°‡∏≤
            li_elements = container.select('li .f6b6d2a959')
            for li in li_elements:
                txt = li.get_text(strip=True)
                if txt: items.append(txt)
                
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà 2: ‡πÑ‡∏°‡πà‡∏°‡∏µ list ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡πÉ‡∏ï‡πâ h3 (‡πÄ‡∏ä‡πà‡∏ô ‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï, ‡∏ó‡∏µ‡πà‡∏à‡∏≠‡∏î‡∏£‡∏ñ)
            # ‡∏à‡∏≤‡∏Å HTML: <div class="b99b6ef58f fb14de7f14 fdf31a9fa1">...</div>
            if not items:
                desc_tag = container.select_one('.fdf31a9fa1')
                if desc_tag:
                    items.append(desc_tag.get_text(strip=True))

            if items:
                data['categories'].append({
                    "name": category_name,
                    "items": list(set(items))
                })

        return data

    @staticmethod
    def _parse_policies_logic(soup):
        print("   üìã Scanning House Rules (Raw Mode)...")
        policies = [] # üëà ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å Dict {} ‡πÄ‡∏õ‡πá‡∏ô List []
        
        box = soup.select_one('#hp_policies_box')
        if not box: return []

        rows = box.select('div.b0400e5749')
        for row in rows:
            header_el = row.select_one('.e7addce19e')
            content_el = row.select_one('.c92998be48')
            
            if header_el and content_el:
                # ‡πÄ‡∏Å‡πá‡∏ö‡∏î‡∏¥‡∏ö‡πÜ ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ß‡πá‡∏ö‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ß‡πà‡∏≤‡πÑ‡∏á ‡πÄ‡∏£‡∏≤‡πÄ‡∏≠‡∏≤‡∏á‡∏±‡πâ‡∏ô
                raw_topic = ScraperUtils.clean_text(header_el.get_text())
                raw_content = ScraperUtils.clean_text(content_el.get_text(" "))
                
                # ‡∏¢‡∏±‡∏î‡∏•‡∏á List ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Map
                policies.append({
                    "topic": raw_topic,
                    "content": raw_content
                })

        return policies

    @staticmethod
    def _parse_nearby_places_logic(soup):
        """Logic ‡πÅ‡∏Å‡∏∞ Nearby Places (‡∏≠‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)"""
        print("   üìç Scanning Nearby Places...")
        nearby_places = []
        poi_blocks = soup.select('div[data-testid="poi-block"], .hp-poi-content-container__column')
        
        for block in poi_blocks:
            header = block.find('h3') or block.find('div', class_='poi-list__title')
            category = ScraperUtils.clean_text(header.get_text()) if header else "General"
            
            for li in block.select('li'):
                texts = [ScraperUtils.clean_text(t.get_text()) for t in li.select('div, span') if ScraperUtils.clean_text(t.get_text())]
                name, dist = "", ""
                for t in texts:
                    if re.search(r'\d.*(?:‡∏Å‡∏°\.|m|km|‡πÄ‡∏°‡∏ï‡∏£)', t): dist = t
                    elif len(t) > 2 and not name: name = t
                
                if name and dist:
                    nearby_places.append({"category": category, "name": name, "distance": dist})
        
        return nearby_places

    @staticmethod
    def _parse_reviews_logic(soup):
        print("   ‚≠ê Scanning Reviews (Direct Array)...")
        
        # Init ‡∏Ñ‡πà‡∏≤ Default
        result = {
            "rating": 0.0,
            "reviewCount": 0,
            "categories": [] 
        }

        # 1. Total Score & Count
        score_comp = soup.select_one('[data-testid="review-score-component"]')
        if score_comp:
            # Rating
            score_val_el = score_comp.select_one('div[aria-hidden="true"]')
            if score_val_el:
                try:
                    result['rating'] = float(score_val_el.get_text().strip())
                except: pass
            
            # Review Count
            count_text = score_comp.get_text()
            match = re.search(r'(\d+)\s*(?:reviews|‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô)', count_text, re.IGNORECASE)
            if match:
                result['reviewCount'] = int(match.group(1))

        # 2. Categories (Loop ‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏á List)
        subscores = soup.select('[data-testid="review-subscore"]')
        for row in subscores:
            name_el = row.select_one('.d96a4619c0, span:first-child') or row.select_one('div:first-child')
            val_el = row.select_one('[aria-hidden="true"]')
            
            if name_el and val_el:
                key = ScraperUtils.clean_text(name_el.get_text())
                try:
                    val = float(ScraperUtils.clean_text(val_el.get_text()))
                    
                    result['categories'].append({
                        "category": key,
                        "rating": val
                    })
                except:
                    continue

        return result

    @staticmethod
    def _parse_images_logic(soup):
        """
        Hydra Photo Miner üêâ (Headed & Lazy-load Proof)
        """
        print("   üñºÔ∏è  Scanning Images (Hardcore Mode)...")
        images = []

        # 1. ‡πÄ‡∏•‡πá‡∏á‡πÄ‡∏õ‡πâ‡∏≤‡∏ó‡∏µ‡πà Gallery ‡∏Å‡πâ‡∏≠‡∏ô‡πÉ‡∏´‡∏ç‡πà
        gallery = soup.select_one('[data-testid="GalleryUnifiedDesktop-wrapper"]')
        if not gallery:
            gallery = soup.select_one('#photo_wrapper')

        if gallery:
            # 2. ‡∏Å‡∏ß‡∏≤‡∏î‡∏ó‡∏∏‡∏Å‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà URL ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÅ‡∏≠‡∏ö‡∏≠‡∏¢‡∏π‡πà (‡πÅ‡∏á‡∏∞‡∏ó‡∏µ‡∏•‡∏∞‡πÉ‡∏ö)
            for img_tag in gallery.select('img'):
                # ‡πÅ‡∏á‡∏∞ src, data-lazy, data-src ‡πÅ‡∏•‡∏∞ srcset
                candidates = [
                    img_tag.get('src'),
                    img_tag.get('data-lazy'),
                    img_tag.get('data-src'),
                    img_tag.get('srcset', '').split(',')[0].split(' ')[0] if img_tag.get('srcset') else None
                ]
                
                for src in candidates:
                    if src and 'http' in src and '.jpg' in src:
                        # üöÄ ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô High-Res 1280x900 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                        high_res = re.sub(r'max\d+[x\d+]*', 'max1280x900', src)
                        
                        # ‡∏Ñ‡∏•‡∏µ‡∏ô URL ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ k (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡πÄ‡∏™‡∏µ‡∏¢)
                        if '?' in high_res:
                            base = high_res.split('?')[0]
                            k_param = re.search(r'k=[a-f0-9]+', high_res)
                            high_res = f"{base}?{k_param.group(0)}" if k_param else base
                        
                        images.append(high_res)

        # 3. ‡∏•‡∏ö‡∏£‡∏π‡∏õ‡∏ã‡πâ‡∏≥
        final_images = list(dict.fromkeys(images))
        return final_images
    
    @staticmethod
    def parse_main_page(html):
        """Main entry point"""
        soup = BeautifulSoup(html, "html.parser")
        
        data = {
            "description": "", "address": "", "priceDaily": 0,
            "bedrooms": 0, "bathrooms": 0, "maxGuests": 2,
            "images": [], "locationHierarchy": [],
            "latitude": None, "longitude": None,
            "features": {},   
            "facilities": {}, 
            "policies": {},
            "nearbyPlaces": [],
            "rating": 0.0,
            "reviewCount": 0,
            "reviewData": []
        }

        # --- ‚ö° CALL ALL SUB-PARSERS ---
        data['features'] = BookingParser._parse_room_table_logic(soup)
        data['facilities'] = BookingParser._parse_facilities_logic(soup)
        data['policies'] = BookingParser._parse_policies_logic(soup)
        data['nearbyPlaces'] = BookingParser._parse_nearby_places_logic(soup) # ‚úÖ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        # -------------------------------

        # Basic Info
        desc_el = soup.select_one('[data-testid="property-description"], #property_description_content')
        if desc_el: data['description'] = ScraperUtils.clean_text(desc_el.get_text(" "))

        # Address
        addr_el = None
        header_wrapper = soup.select_one('[data-testid="PropertyHeaderAddressDesktop-wrapper"]')
        if header_wrapper:
            target_text = header_wrapper.find(string=re.compile("‡πÑ‡∏ó‡∏¢"))
            if target_text and target_text.parent:
                addr_container = target_text.parent
                for hidden in addr_container.select('[aria-hidden="true"]'): hidden.decompose()
                addr_el = addr_container
        if not addr_el: addr_el = soup.select_one('[data-node_tt_id="location_score_tooltip"]')
        if not addr_el: addr_el = soup.select_one('.hp_address_subtitle')

        if addr_el:
            raw_addr = ScraperUtils.clean_text(addr_el.get_text(" ")) 
            for bad in ["‡πÅ‡∏™‡∏î‡∏á‡∏ö‡∏ô‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà", "Show on map", "‡∏ó‡∏≥‡πÄ‡∏•‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°", "Excellent location", "‚Äì"]:
                raw_addr = raw_addr.replace(bad, "")
            data['address'] = re.sub(r'\s+,\s+', ', ', raw_addr).strip()

        # Images
        data['images'] = BookingParser._parse_images_logic(soup)

        # Coordinates
        map_link = soup.select_one('a[data-atlas-latlng]')
        if map_link:
            try:
                lat, lng = map_link.get('data-atlas-latlng').split(',')
                data['latitude'], data['longitude'] = float(lat), float(lng)
            except: pass

        # Reviews
        reviews = BookingParser._parse_reviews_logic(soup)
        data['rating'] = reviews['rating']
        data['reviewCount'] = reviews['reviewCount']
        data['reviewData'] = reviews['categories']
            
        return data

# ==========================================
# 4. WORKER (Updated for Nearby Places)
# ==========================================

async def process_villa(sem, context, villa):
    async with sem:
        page = await context.new_page()
        await ScraperUtils.apply_stealth(page)

        try:
            future_date = datetime.now() + timedelta(days=120) 
            
            checkin = future_date.strftime('%Y-%m-%d')
            checkout = (future_date + timedelta(days=1)).strftime('%Y-%m-%d')
            
            target_url = (
                f"{villa.sourceUrl}?"
                f"lang=th&selected_currency=THB"
                f"&checkin={checkin}&checkout={checkout}"
                f"&group_adults=2&no_rooms=1"
            )

            print(f"   ‚õèÔ∏è  Opening: {villa.slug}")
            await page.goto(target_url, timeout=CONFIG["TIMEOUT"], wait_until="domcontentloaded")
            
            try:
                await page.wait_for_selector('[data-testid="property-description"]', timeout=15000)
            except:
                print(f"      ‚ö†Ô∏è Timeout (Content Load): {villa.slug}")
                await page.close()
                return

            # await page.evaluate("window.scrollTo(0, 3000)") 
            await page.evaluate("""
                const el = document.getElementById('hp_facilities_box');
                if (el) el.scrollIntoView();
            """)
            
            try:
                await page.wait_for_selector('div[data-testid="facility-group-container"]', state="visible", timeout=10000)
            except:
                print("      ‚ö†Ô∏è Facilities not visible, scrolling might be needed...")

            # --- PARSING ---
            content = await page.content()
            data = BookingParser.parse_main_page(content)

            # --- SAVING ---
            async with AsyncSessionLocal() as session:
                room_specs = data['features'].get('specs', {})
                room_price = data['features'].get('priceDaily', 0)

                stmt = update(Villa).where(Villa.id == villa.id).values(
                    # description=data.get('description'),
                    # address=data.get('address') or Villa.address,
                    # latitude=data.get('latitude'),
                    # longitude=data.get('longitude'),
                    images=data.get('images'),
                    # features=data.get('features'),   
                    facilities=data.get('facilities'), 
                    # policies=data.get('policies'),
                    nearbyPlaces=data.get('nearbyPlaces'), 
                    # rating=data.get('rating'),
                    # reviewCount=data.get('reviewCount'),
                    # reviewData=data.get('reviewData'),
                    # priceDaily=data.get('priceDaily') or room_price or Villa.priceDaily,
                    # bedrooms=data.get('bedrooms') or room_specs.get('bedrooms') or Villa.bedrooms,
                    # bathrooms=data.get('bathrooms') or room_specs.get('bathrooms') or Villa.bathrooms,
                    # maxGuests=data.get('maxGuests') or room_specs.get('maxGuests') or Villa.maxGuests,
                    updatedAt=func.now()
                )
                await session.execute(stmt)
                await session.commit()
            print(f"      ‚úÖ Saved: {villa.slug}")

        except Exception as e:
            print(f"      ‚ùå Error in {villa.slug}: {str(e)}")
        finally:
            await page.close()

async def run_enricher():
    print("üåô Hydra Miner V3.5 (Refactored) Started...")
    
    async with async_playwright() as p:

        browser = await p.chromium.launch(headless=True) # Headless ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö
        
        context = await browser.new_context(
            user_agent=CONFIG["USER_AGENT"],
            locale="th-TH"
        )

        sem = asyncio.Semaphore(CONFIG["CONCURRENT_TABS"])

        while True:
            async with AsyncSessionLocal() as session:
                stmt = select(Villa).where(
                    (Villa.images == cast([], JSONB)) | (Villa.images == None)
                ).limit(10)
                # stmt = select(Villa).where(
                #     Villa.id.in_([6473, 4331, 6378])
                # ).limit(10)
                result = await session.execute(stmt)
                villas = result.scalars().all()

                if not villas:
                    print("üéâ Mission Complete! No more villas to process.")
                    break

                print(f"\nüöÄ Processing Batch: {len(villas)} villas")
                
                tasks = [process_villa(sem, context, v) for v in villas]
                await asyncio.gather(*tasks)
                
                print(f"--- Batch Finished ---")
                await asyncio.sleep(2)

                # print("üõë Test run complete. Stopping script.")
                # break

        await browser.close()

if __name__ == "__main__":
    asyncio.run(run_enricher())